{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RosaliaGimenezBDS/M9_Reto_01/blob/main/Reto_1_M09_Notebook_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dG2MLbQvZy9h"
      },
      "source": [
        "# **Máster en Behavioral Data Science**\n",
        "## **Instituto de Formación Continua (IL3) - Universitat de Barcelona**\n",
        "## **Módulo 9: Aprendizaje Profundo - Reto 1** - (Notebook 5/5)\n",
        "Autores: **Meysam Madadi** & **Julio C. S. Jacques Junior**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Prerrequisitos**\n",
        "- Consultar las instrucciones en los *Jupyter notebooks* anteriores.\n",
        "- Ejecutar los *Jupyter notebooks* anteriores, en este caso:\n",
        " - *Jupyter notebook* 1.\n",
        " - *Jupyter notebook* 2.\n",
        " - *Jupyter notebook* 3.\n",
        " - *Jupyter notebook* 4."
      ],
      "metadata": {
        "id": "6azMhdpv2Ylf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Los objetivos de este Jupyter notebook**\n",
        "- **Evaluar una estrategia diferente para la mitigación de sesgos que no dependa del aumento de datos**, basada en una **función de pérdida ponderada**.\n",
        "- Refinaremos el modelo obtenido al final de la Etapa 1 usando una segunda etapa, pero ahora con esta nueva función de pérdida ponderada.\n",
        "- Luego, evaluaremos los de sesgos y compararemos los resultados.\n",
        "---"
      ],
      "metadata": {
        "id": "hsOOAmlq2c7S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comprobando la versión de tensorflow"
      ],
      "metadata": {
        "id": "RukF8nvQ47tv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Este código fue probado en tensorflow 2.15.0\n",
        "#import tensorflow as tf\n",
        "import tensorflow as tf\n",
        "import tf_keras\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "id": "TQw-JWZn4-NL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e270f2c-cf07-4d6b-ad1f-c2758908d94a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.17.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwG95hLURSGj"
      },
      "source": [
        "# Montando nuestro Google Drive para guardar/cargar nuestros resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7sByyqKRae5"
      },
      "outputs": [],
      "source": [
        "#--------------------------\n",
        "MOUNT_GOOGLE_DRIVE = True\n",
        "#--------------------------\n",
        "\n",
        "if(MOUNT_GOOGLE_DRIVE==True):\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/gdrive')\n",
        "  # Note, the default path will be: '/content/gdrive/MyDrive/'\n",
        "  # In my case, the final path will be: '/content/gdrive/MyDrive/M09-P01/' as I\n",
        "  # created a '/M09-P01/' folder in my google drive for this purpose."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cargando el modelo y los datos preprocesados desde Drive"
      ],
      "metadata": {
        "id": "YztHVxmckNEz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "#import random\n",
        "#import gc\n",
        "\n",
        "with open('/content/gdrive/MyDrive/M09-P01/train.npy', 'rb') as f:\n",
        "  X_train = np.load(f)\n",
        "  Y_train = np.load(f)\n",
        "  M_train = np.load(f)\n",
        "with open('/content/gdrive/MyDrive/M09-P01/valid.npy', 'rb') as f:\n",
        "  X_valid = np.load(f)\n",
        "  Y_valid = np.load(f)\n",
        "  M_valid = np.load(f)\n",
        "with open('/content/gdrive/MyDrive/M09-P01/test.npy', 'rb') as f:\n",
        "  X_test = np.load(f)\n",
        "  Y_test = np.load(f)\n",
        "  M_test = np.load(f)\n",
        "\n",
        "# the model is loaded in the next cell"
      ],
      "metadata": {
        "id": "thG5tblfkR-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjkJNXGilNfo"
      },
      "source": [
        "# **Mitigando el problema de sesgo utilizando una función de pérdida ponderada** (*weighted loss*).\n",
        "- A continuación, crearemos una **\"función de pérdida personalizada\", que da más peso a las personas que tienen menos muestras en los datos de entrenamiento**. De esta forma, la red intentará predecir mejor sus edades al intentar minimizar la pérdida total, \"minimizando\" el problema de desbalance de datos.\n",
        "- Para este ejemplo, **consideraremos solo el atributo de edad**, pero se podrían considerar otras categorías, así como soluciones más sofisticadas.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGjU8Xetxl3W"
      },
      "source": [
        "# **Generando los pesos para las muestras de entrenamiento**\n",
        "- Primero, generaremos un peso para cada grupo de edad (para g=1 a 4, es decir, \"0-19\", \"20-39\", \"40-59\" y \"60-100\");\n",
        "- La fórmula utilizada para calcular el peso de cada grupo $j$ es:\n",
        "\n",
        "  $w_j=n_{samples} / (n_{classes} * n_{samples,j}),$\n",
        "\n",
        "  Dónde\n",
        "\n",
        "     - $w_j$ es el peso de cada grupo $j$,\n",
        "     - $n_{samples}$ es el número de muestras en el conjunto de entrenamiento,\n",
        "     - $n_{clases}$ es el número de clases (4 en nuestro caso, ya que hemos dividido las edades en 4 grupos),\n",
        "     - $n_{samples,j}$ es el número de muestras de la clase $j$.\n",
        "- Durante el entrenamiento, le pasamos a *kereas* un vector de pesos, y la función de pérdida se calculará teniendo en cuenta el peso de cada grupo y la respectiva etiqueta de la muestra de entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svMMpvU9xcJR"
      },
      "outputs": [],
      "source": [
        "# counting the number of samples per group in the train data (age attribute only)\n",
        "g1 = g2 = g3 = g4 = 0\n",
        "for i in range(0,Y_train.shape[0]):\n",
        "    if(Y_train[i]*100<20):\n",
        "      g1 +=1\n",
        "    if(Y_train[i]*100>=20 and Y_train[i]*100<40):\n",
        "      g2 +=1\n",
        "    if(Y_train[i]*100>=40 and Y_train[i]*100<60):\n",
        "      g3 +=1\n",
        "    if(Y_train[i]*100>=60):\n",
        "      g4 +=1\n",
        "print('group(s) size = ', [g1, g2, g3, g4])\n",
        "\n",
        "# generating the weights for each group using the equation defined above\n",
        "w = sum(np.array([g1, g2, g3, g4]))/(4*np.array([g1, g2, g3, g4]))\n",
        "print('weights per group = ', w)\n",
        "\n",
        "# creating a vector with same size as Y_train, that will link a particular label to its weight\n",
        "sample_weights = []\n",
        "for i in range(0,Y_train.shape[0]):\n",
        "    if(Y_train[i]*100<20):\n",
        "      sample_weights.append(w[0])\n",
        "    if(Y_train[i]*100>=20 and Y_train[i]*100<40):\n",
        "      sample_weights.append(w[1])\n",
        "    if(Y_train[i]*100>=40 and Y_train[i]*100<60):\n",
        "      sample_weights.append(w[2])\n",
        "    if(Y_train[i]*100>=60):\n",
        "      sample_weights.append(w[3])\n",
        "sample_weights = np.array(sample_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gTtMXGmx5jA"
      },
      "source": [
        "# **Refinando nuestro modelo con una función de pérdida ponderada**\n",
        "- Ahora, en lugar de simplemente ajustar el modelo aprendido en la etapa 1 con la segunda etapa, **ajustamos el modelo aprendido en la etapa 1 con la segunda etapa, nuestra función de pérdida ponderada (y sin aumento de datos)**. De esta manera, podemos comparar las diferentes estrategias."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# LOADING THE PREVIOUSLY TRAINED MODEL\n",
        "model = tf_keras.models.load_model('/content/gdrive/MyDrive/M09-P01/best_model_st1.h5')\n",
        "\n",
        "# setting all layers of the model to trainable\n",
        "model.trainable = True\n",
        "\n",
        "#### MODEL TRAINING ####\n",
        "# defining the early stop criteria\n",
        "es = tf_keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
        "# saving the best model based on val_loss\n",
        "mc = tf_keras.callbacks.ModelCheckpoint('/content/gdrive/MyDrive/M09-P01/best_model_st2_custom_loss.h5', monitor='val_loss', mode='min', save_best_only=True)\n",
        "\n",
        "# defining the optimizer\n",
        "model.compile(tf_keras.optimizers.Adam(learning_rate=1e-5),loss=tf_keras.losses.MeanSquaredError(),metrics=['mae'])\n",
        "\n",
        "# training the model\n",
        "history = model.fit(X_train, Y_train, sample_weight=sample_weights, validation_data=(X_valid, Y_valid), batch_size=32, epochs=50, shuffle=True, verbose=1, callbacks=[es,mc])\n",
        "\n",
        "# saving training history (for future visualization)\n",
        "with open('/content/gdrive/MyDrive/M09-P01/train_history_st2_custom_loss.pkl', 'wb') as handle:\n",
        "  pickle.dump(history.history, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
      ],
      "metadata": {
        "id": "yVy94XTi0T_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluando el modelo entrenado (Etapa 2, con una función de pérdida ponderada) en el conjunto de prueba"
      ],
      "metadata": {
        "id": "TryU6F492_Sa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#loading the best model\n",
        "model = tf_keras.models.load_model('/content/gdrive/MyDrive/M09-P01/best_model_st2_custom_loss.h5')\n",
        "\n",
        "# Evaluate the trained model on the test set\n",
        "print('Evaluating on the test set')\n",
        "predictions_st2_custom_loss = model.predict(X_test, batch_size=32, verbose=1)\n",
        "\n",
        "# Computing the Mean Absolute Error\n",
        "# Also re-scaling the predictions to the range of \"age\" as the outputs are in the range of [0,1]\n",
        "mae = np.mean(abs(predictions_st2_custom_loss[:,0] - Y_test)*100)\n",
        "\n",
        "# Next we print the average error. Note that the error is rescaled back to the range [0-100]\n",
        "print('\\nThe final mean absolute error (on the Test set)  is ' + str(mae) + ' years old.')"
      ],
      "metadata": {
        "id": "NWCsgr6r3GvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpJCmvGQXDIS"
      },
      "source": [
        "# **Comparando ambos modelos en términos de valores de sesgo**\n",
        "- A continuación, evaluamos los valores de sesgo obtenidos por cada modelo:\n",
        " - Etapa 2 (estándar);\n",
        " - Etapa 2, con una función de pérdida ponderada."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# downloading our \"bias library\", which contains the functions used to evaluate\n",
        "# the different bias scores\n",
        "!wget http://data.chalearnlap.cvc.uab.cat/Colab_MFPDS/2024BehaviorDSMaster/M9_r1/bias_functions.py"
      ],
      "metadata": {
        "id": "VtroHPd0ACMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YjrDG9AHLoPf"
      },
      "outputs": [],
      "source": [
        "# importing the functions used to evaluate the different biases\n",
        "from bias_functions import age_bias, gender_bias, ethnicity_bias, face_expression_bias\n",
        "\n",
        "# loading the last saved model and perform prediction\n",
        "model = tf_keras.models.load_model('/content/gdrive/MyDrive/M09-P01/best_model_st2.h5')\n",
        "predictions_st2 = model.predict(tf.convert_to_tensor(X_test, dtype=tf.float32), batch_size=32, verbose=0)\n",
        "\n",
        "age_bias(predictions_st2*100, Y_test*100)\n",
        "age_bias(predictions_st2_custom_loss*100, Y_test*100)\n",
        "\n",
        "gender_bias(predictions_st2*100, Y_test*100, M_test)\n",
        "gender_bias(predictions_st2_custom_loss*100, Y_test*100, M_test)\n",
        "\n",
        "ethnicity_bias(predictions_st2*100, Y_test*100, M_test)\n",
        "ethnicity_bias(predictions_st2_custom_loss*100, Y_test*100, M_test)\n",
        "\n",
        "face_expression_bias(predictions_st2*100, Y_test*100, M_test)\n",
        "face_expression_bias(predictions_st2_custom_loss*100, Y_test*100, M_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualizando el historial de entrenamiento usando diferentes estrategias\n",
        "- De manera similar a lo observado en el *Jupyter notebook* 4, las curvas de entrenamiento de ambos modelos muestran un comportamiento similar. Sin embargo, creemos que la nueva función de pérdida podría haber contribuido a mitigar el problema de sesgo, y esto puede analizarse de manera cuantitativa.\n",
        "- En general, los resultados pueden variar un poco según diferentes ejecuciones, debido a factores de inicialización, pero **pueden darnos una idea de lo que se puede hacer para mejorar las capacidades de generalización (y equidad) de nuestro modelo**."
      ],
      "metadata": {
        "id": "nsIBx9DzBt5j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "train_hist2 = pickle.load(open('/content/gdrive/MyDrive/M09-P01/train_history_st2.pkl',\"rb\"))\n",
        "train_hist3 = pickle.load(open('/content/gdrive/MyDrive/M09-P01/train_history_st2_custom_loss.pkl',\"rb\"))\n",
        "train_hist4 = pickle.load(open('/content/gdrive/MyDrive/M09-P01/train_history_st2_aug.pkl',\"rb\"))\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 4))\n",
        "fig.suptitle('Training history (Stage 2 vs. Stage 2 with custom loss)', fontsize=14, fontweight='bold')\n",
        "\n",
        "ax1.plot(train_hist2['loss'])\n",
        "ax1.plot(train_hist3['loss'])\n",
        "ax1.plot(train_hist2['val_loss'])\n",
        "ax1.plot(train_hist3['val_loss'])\n",
        "ax1.set(xlabel='epoch', ylabel='Loss')\n",
        "ax1.legend(['train', 'train custom', 'valid', 'valid cutsom'], loc='upper right')\n",
        "\n",
        "ax2.plot(train_hist2['mae'])\n",
        "ax2.plot(train_hist3['mae'])\n",
        "ax2.plot(train_hist2['val_mae'])\n",
        "ax2.plot(train_hist3['val_mae'])\n",
        "ax2.set(xlabel='epoch', ylabel='MAE')\n",
        "ax2.legend(['train', 'train custom', 'valid', 'valid custom'], loc='upper right')"
      ],
      "metadata": {
        "id": "PhRwp8u0EkuV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}